{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xVuzE1IAvuBK"
      },
      "source": [
        "# Theni Tamil Dialect Translator — mBART Fine-tune\n",
        "**Before running:**\n",
        "1. Runtime → Disconnect and delete runtime\n",
        "2. Runtime → Change runtime type → T4 GPU → Save\n",
        "3. Run each cell top to bottom"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0GC-drhgvuBP"
      },
      "outputs": [],
      "source": [
        "# CELL 1 - Install\n",
        "!pip install -q transformers sentencepiece peft accelerate sacrebleu\n",
        "print('Done')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0mHlp6wBvuBR"
      },
      "outputs": [],
      "source": [
        "# CELL 2 - Imports\n",
        "import os, gc, io, torch, shutil, math\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "from transformers import (\n",
        "    MBartForConditionalGeneration, MBart50Tokenizer,\n",
        "    get_cosine_schedule_with_warmup, DataCollatorForSeq2Seq\n",
        ")\n",
        "from peft import get_peft_model, LoraConfig, TaskType, PeftModel\n",
        "from tqdm.notebook import tqdm\n",
        "import sacrebleu   # pip install sacrebleu\n",
        "\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f'Device: {device}')\n",
        "if torch.cuda.is_available():\n",
        "    free  = torch.cuda.mem_get_info()[0] / 1e9\n",
        "    total = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
        "    print(f'GPU: {torch.cuda.get_device_name(0)}')\n",
        "    print(f'VRAM: {total:.1f} GB total | {free:.1f} GB free')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VblG6m8UvuBT"
      },
      "outputs": [],
      "source": [
        "# CELL 3 - Upload CSV\n",
        "from google.colab import files\n",
        "print('Upload your dataset.csv ...')\n",
        "uploaded = files.upload()\n",
        "filename = list(uploaded.keys())[0]\n",
        "df_raw   = pd.read_csv(io.BytesIO(uploaded[filename]))\n",
        "print(f'Uploaded: {filename}')\n",
        "print(f'Rows: {len(df_raw)}')\n",
        "print(f'Columns: {list(df_raw.columns)}')\n",
        "df_raw.head(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7bJxTICGvuBU"
      },
      "outputs": [],
      "source": [
        "# CELL 4 - Settings\n",
        "SOURCE_COL = 'normal_tamil'   # change if your CSV column is named differently\n",
        "TARGET_COL = 'theni_tamil'    # change if your CSV column is named differently\n",
        "\n",
        "MODEL_NAME = 'facebook/mbart-large-50'\n",
        "SRC_LANG   = 'ta_IN'\n",
        "TGT_LANG   = 'ta_IN'\n",
        "\n",
        "MAX_LENGTH         = 128\n",
        "BATCH_SIZE         = 8          # physical batch size (fits T4)\n",
        "GRAD_ACCUM_STEPS   = 4          # effective batch = 8 × 4 = 32\n",
        "EPOCHS             = 30         # more epochs with early stopping\n",
        "LR                 = 5e-5       # lower LR → stabler training\n",
        "LABEL_SMOOTHING    = 0.1        # regularisation\n",
        "PATIENCE           = 5          # early-stop if val loss doesn't improve\n",
        "MODEL_DIR          = '/content/best_model'\n",
        "\n",
        "df = df_raw[[SOURCE_COL, TARGET_COL]].dropna()\n",
        "df[SOURCE_COL] = df[SOURCE_COL].str.strip()\n",
        "df[TARGET_COL] = df[TARGET_COL].str.strip()\n",
        "df = df[(df[SOURCE_COL].str.len() > 0) & (df[TARGET_COL].str.len() > 0)]\n",
        "\n",
        "# ── Data augmentation: duplicate rare / short pairs ─────────────────────────\n",
        "df_aug = df.copy()\n",
        "# Duplicate pairs where source has fewer than 5 words (harder short sentences)\n",
        "short_mask = df[SOURCE_COL].str.split().str.len() < 5\n",
        "df_aug = pd.concat([df, df[short_mask]], ignore_index=True)\n",
        "print(f'Original pairs: {len(df)} | After augmentation: {len(df_aug)}')\n",
        "\n",
        "src_train, src_val, tgt_train, tgt_val = train_test_split(\n",
        "    df_aug[SOURCE_COL].tolist(), df_aug[TARGET_COL].tolist(),\n",
        "    test_size=0.1, random_state=42\n",
        ")\n",
        "print(f'Train: {len(src_train)} | Val: {len(src_val)}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JQ1i0dOCvuBV"
      },
      "outputs": [],
      "source": [
        "# CELL 5 - Load mBART + LoRA\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "print('Loading tokenizer...')\n",
        "tokenizer = MBart50Tokenizer.from_pretrained(MODEL_NAME)\n",
        "tokenizer.src_lang = SRC_LANG\n",
        "tokenizer.tgt_lang = TGT_LANG\n",
        "\n",
        "print('Loading base model in fp16...')\n",
        "base_model = MBartForConditionalGeneration.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    torch_dtype=torch.float16\n",
        ")\n",
        "base_model.config.use_cache = False\n",
        "\n",
        "print('Applying LoRA...')\n",
        "lora_config = LoraConfig(\n",
        "    task_type=TaskType.SEQ_2_SEQ_LM,\n",
        "    r=32,                          # ↑ from 16 → 32 (more capacity)\n",
        "    lora_alpha=64,                 # keep alpha = 2×r\n",
        "    lora_dropout=0.05,             # small dropout\n",
        "    # ── target BOTH encoder AND decoder attention + FFN gates ────────────\n",
        "    target_modules=[\n",
        "        'q_proj', 'k_proj', 'v_proj', 'out_proj',   # attention\n",
        "        'fc1', 'fc2'                                  # feed-forward\n",
        "    ],\n",
        "    bias='none',\n",
        ")\n",
        "model = get_peft_model(base_model, lora_config)\n",
        "\n",
        "# Cast LoRA params to float32 for stable gradients with fp16 base\n",
        "for name, param in model.named_parameters():\n",
        "    if param.requires_grad:\n",
        "        param.data = param.data.float()\n",
        "\n",
        "model.to(device)\n",
        "\n",
        "total     = sum(p.numel() for p in model.parameters())\n",
        "trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f'Total params    : {total/1e6:.1f}M')\n",
        "print(f'Trainable params: {trainable/1e6:.2f}M ({100*trainable/total:.2f}%)')\n",
        "free = torch.cuda.mem_get_info()[0] / 1e9\n",
        "print(f'Free VRAM       : {free:.1f} GB')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bcWnFQagvuBW"
      },
      "outputs": [],
      "source": [
        "# CELL 6 - Dataset  (dynamic padding via DataCollatorForSeq2Seq)\n",
        "class TheniDataset(Dataset):\n",
        "    def __init__(self, sources, targets, tokenizer, max_length=128):\n",
        "        self.sources   = sources\n",
        "        self.targets   = targets\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len   = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.sources)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Encode source\n",
        "        self.tokenizer.src_lang = SRC_LANG\n",
        "        model_inputs = self.tokenizer(\n",
        "            str(self.sources[idx]),\n",
        "            max_length=self.max_len,\n",
        "            truncation=True,\n",
        "        )\n",
        "        # Encode target: MBart50Tokenizer uses tgt_lang to set the BOS token\n",
        "        self.tokenizer.src_lang = TGT_LANG\n",
        "        labels = self.tokenizer(\n",
        "            str(self.targets[idx]),\n",
        "            max_length=self.max_len,\n",
        "            truncation=True,\n",
        "        )\n",
        "        self.tokenizer.src_lang = SRC_LANG  # restore\n",
        "        model_inputs['labels'] = labels['input_ids']\n",
        "        return model_inputs\n",
        "\n",
        "# Dynamic padding collator — pads each batch to its longest sequence\n",
        "collator = DataCollatorForSeq2Seq(\n",
        "    tokenizer,\n",
        "    model=model,\n",
        "    label_pad_token_id=-100,   # -100 is ignored in loss\n",
        "    pad_to_multiple_of=8,      # good for tensor-core alignment\n",
        ")\n",
        "\n",
        "train_dataset = TheniDataset(src_train, tgt_train, tokenizer, MAX_LENGTH)\n",
        "val_dataset   = TheniDataset(src_val,   tgt_val,   tokenizer, MAX_LENGTH)\n",
        "train_loader  = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True,\n",
        "                           collate_fn=collator, num_workers=2, pin_memory=True)\n",
        "val_loader    = DataLoader(val_dataset,   batch_size=BATCH_SIZE, shuffle=False,\n",
        "                           collate_fn=collator, num_workers=2, pin_memory=True)\n",
        "print(f'Steps/epoch: {len(train_loader)}  |  Eff. batch size: {BATCH_SIZE * GRAD_ACCUM_STEPS}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1XpCHEyzvuBY"
      },
      "outputs": [],
      "source": [
        "# CELL 7 - Train  (label smoothing + cosine LR + gradient accumulation + early stopping + BLEU)\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def smooth_loss(logits, labels, eps=LABEL_SMOOTHING):\n",
        "    \"\"\"Cross-entropy with label smoothing; ignores -100 positions.\"\"\"\n",
        "    vocab_size = logits.size(-1)\n",
        "    # flat\n",
        "    logits_flat = logits.view(-1, vocab_size)\n",
        "    labels_flat = labels.view(-1)\n",
        "\n",
        "    mask = labels_flat != -100\n",
        "    logits_flat = logits_flat[mask]\n",
        "    labels_flat = labels_flat[mask]\n",
        "\n",
        "    log_probs = F.log_softmax(logits_flat, dim=-1)\n",
        "    nll = F.nll_loss(log_probs, labels_flat, reduction='mean')\n",
        "    smooth = -log_probs.mean()\n",
        "    return (1 - eps) * nll + eps * smooth\n",
        "\n",
        "total_steps  = len(train_loader) // GRAD_ACCUM_STEPS * EPOCHS\n",
        "warmup_steps = total_steps // 10   # 10 % warm-up\n",
        "\n",
        "optimizer = torch.optim.AdamW(\n",
        "    [p for p in model.parameters() if p.requires_grad],\n",
        "    lr=LR, weight_decay=0.01, betas=(0.9, 0.98)\n",
        ")\n",
        "scheduler = get_cosine_schedule_with_warmup(\n",
        "    optimizer,\n",
        "    num_warmup_steps=warmup_steps,\n",
        "    num_training_steps=total_steps\n",
        ")\n",
        "\n",
        "os.makedirs(MODEL_DIR, exist_ok=True)\n",
        "best_val_loss   = float('inf')\n",
        "patience_count  = 0\n",
        "history         = []\n",
        "\n",
        "print(f'Training {EPOCHS} epochs | {total_steps} total opt-steps | '\n",
        "      f'{warmup_steps} warmup steps')\n",
        "\n",
        "for epoch in range(1, EPOCHS + 1):\n",
        "    # ── Train ───────────────────────────────────────────────────────────────\n",
        "    model.train()\n",
        "    total_train = 0\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    for step, batch in enumerate(tqdm(train_loader, desc=f'Epoch {epoch}/{EPOCHS} [Train]')):\n",
        "        input_ids      = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels         = batch['labels'].to(device)\n",
        "\n",
        "        outputs = model(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            labels=labels\n",
        "        )\n",
        "        # Use label-smoothed loss instead of model's raw CE\n",
        "        loss = smooth_loss(outputs.logits, labels) / GRAD_ACCUM_STEPS\n",
        "        total_train += loss.item() * GRAD_ACCUM_STEPS\n",
        "        loss.backward()\n",
        "\n",
        "        if (step + 1) % GRAD_ACCUM_STEPS == 0:\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "        del input_ids, attention_mask, labels, outputs, loss\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    # ── Validate  (loss + BLEU/chrF) ────────────────────────────────────────\n",
        "    model.eval()\n",
        "    total_val    = 0\n",
        "    hyps, refs   = [], []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(val_loader, desc=f'Epoch {epoch}/{EPOCHS} [Val]'):\n",
        "            input_ids      = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels         = batch['labels'].to(device)\n",
        "\n",
        "            outputs = model(\n",
        "                input_ids=input_ids,\n",
        "                attention_mask=attention_mask,\n",
        "                labels=labels\n",
        "            )\n",
        "            total_val += smooth_loss(outputs.logits, labels).item()\n",
        "\n",
        "            # ── Generate for BLEU (every epoch) ─────────────────────────────\n",
        "            forced_bos = tokenizer.lang_code_to_id[TGT_LANG]\n",
        "            gen_ids = model.generate(\n",
        "                input_ids=input_ids,\n",
        "                attention_mask=attention_mask,\n",
        "                forced_bos_token_id=forced_bos,\n",
        "                num_beams=4,\n",
        "                max_length=MAX_LENGTH,\n",
        "                early_stopping=True\n",
        "            )\n",
        "            decoded_hyp = tokenizer.batch_decode(gen_ids, skip_special_tokens=True)\n",
        "            # Recover reference strings (replace -100 back to pad_id for decoding)\n",
        "            ref_ids = labels.clone()\n",
        "            ref_ids[ref_ids == -100] = tokenizer.pad_token_id\n",
        "            decoded_ref = tokenizer.batch_decode(ref_ids, skip_special_tokens=True)\n",
        "\n",
        "            hyps.extend(decoded_hyp)\n",
        "            refs.extend(decoded_ref)\n",
        "\n",
        "            del input_ids, attention_mask, labels, outputs, gen_ids\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "    avg_train = total_train / len(train_loader)\n",
        "    avg_val   = total_val   / len(val_loader)\n",
        "\n",
        "    bleu_score = sacrebleu.corpus_bleu(hyps, [refs]).score\n",
        "    chrf_score = sacrebleu.corpus_chrf(hyps, [refs]).score\n",
        "\n",
        "    history.append({'epoch': epoch, 'train': avg_train, 'val': avg_val,\n",
        "                    'bleu': bleu_score, 'chrf': chrf_score})\n",
        "\n",
        "    print(f'Epoch {epoch:02d} | Train: {avg_train:.4f} | Val: {avg_val:.4f} '\n",
        "          f'| BLEU: {bleu_score:.2f} | chrF: {chrf_score:.2f}')\n",
        "\n",
        "    if avg_val < best_val_loss:\n",
        "        best_val_loss  = avg_val\n",
        "        patience_count = 0\n",
        "        model.save_pretrained(MODEL_DIR)\n",
        "        tokenizer.save_pretrained(MODEL_DIR)\n",
        "        print(f'  ✓ Saved best model (val={best_val_loss:.4f}, BLEU={bleu_score:.2f})')\n",
        "    else:\n",
        "        patience_count += 1\n",
        "        print(f'  No improvement ({patience_count}/{PATIENCE})')\n",
        "        if patience_count >= PATIENCE:\n",
        "            print('Early stopping triggered.')\n",
        "            break\n",
        "\n",
        "print('\\nTraining complete!')\n",
        "print(f'Best val loss: {best_val_loss:.4f}')\n",
        "\n",
        "# ── Print training curve ─────────────────────────────────────────────────────\n",
        "print('\\nEpoch | Train  | Val    | BLEU  | chrF')\n",
        "print('-'*45)\n",
        "for h in history:\n",
        "    print(f\"{h['epoch']:5d} | {h['train']:.4f} | {h['val']:.4f} | {h['bleu']:5.2f} | {h['chrf']:.2f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "znhLP8ZTvuBa"
      },
      "outputs": [],
      "source": [
        "# CELL 8 - Download model\n",
        "from google.colab import files\n",
        "shutil.make_archive('/content/theni_mbart_model', 'zip', MODEL_DIR)\n",
        "files.download('/content/theni_mbart_model.zip')\n",
        "print('Downloaded!')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tdB0pikyvuBb"
      },
      "outputs": [],
      "source": [
        "# CELL 9 - Load model for translation\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "# MBart50Tokenizer must be used (not AutoTokenizer) to preserve lang_code_to_id\n",
        "tokenizer_t = MBart50Tokenizer.from_pretrained(MODEL_DIR, local_files_only=True)\n",
        "tokenizer_t.src_lang = SRC_LANG\n",
        "tokenizer_t.tgt_lang = TGT_LANG\n",
        "\n",
        "base_t  = MBartForConditionalGeneration.from_pretrained(MODEL_NAME, torch_dtype=torch.float16)\n",
        "model_t = PeftModel.from_pretrained(base_t, MODEL_DIR, local_files_only=True)\n",
        "model_t = model_t.merge_and_unload()\n",
        "model_t.to(device)\n",
        "model_t.eval()\n",
        "print('Model ready!')\n",
        "\n",
        "def translate(text):\n",
        "    tokenizer_t.src_lang = SRC_LANG\n",
        "    inputs = tokenizer_t(\n",
        "        text, return_tensors='pt',\n",
        "        max_length=MAX_LENGTH, truncation=True\n",
        "    ).to(device)\n",
        "    # For mBART, forced_bos_token_id must be the target language token id\n",
        "    forced_bos_token_id = tokenizer_t.lang_code_to_id[TGT_LANG]\n",
        "    with torch.no_grad():\n",
        "        out = model_t.generate(\n",
        "            **inputs,\n",
        "            forced_bos_token_id=forced_bos_token_id,\n",
        "            num_beams=5,\n",
        "            max_length=MAX_LENGTH,\n",
        "            early_stopping=True\n",
        "        )\n",
        "    return tokenizer_t.decode(out[0], skip_special_tokens=True)\n",
        "\n",
        "tests = [\n",
        "    'நீங்கள் எப்படி இருக்கிறீர்கள்',\n",
        "    'இன்று மழை பெய்கிறது',\n",
        "    'நான் சாப்பிட போகிறேன்',\n",
        "    'அவர் வீட்டிற்கு வருகிறார்'\n",
        "]\n",
        "print('\\n--- Test Results ---')\n",
        "for t in tests:\n",
        "    print(f'Normal Tamil : {t}')\n",
        "    print(f'Theni Slang  : {translate(t)}')\n",
        "    print('-' * 50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p8o1AgCkvuBd"
      },
      "outputs": [],
      "source": [
        "# CELL 10 - Interactive translation\n",
        "print('Type any Normal Tamil sentence. Type quit to stop.\\n')\n",
        "while True:\n",
        "    text = input('Normal Tamil : ').strip()\n",
        "    if text.lower() == 'quit':\n",
        "        break\n",
        "    if text:\n",
        "        print(f'Theni Slang  : {translate(text)}\\n')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}