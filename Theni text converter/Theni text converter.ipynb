{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required libraries\n",
        "!pip install transformers datasets peft accelerate sentencepiece"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import pandas as pd\n",
        "from datasets import Dataset\n",
        "from transformers import (\n",
        "    MBartForConditionalGeneration,\n",
        "    MBart50TokenizerFast,\n",
        "    TrainingArguments,\n",
        "    Trainer\n",
        ")\n",
        "from peft import LoraConfig, get_peft_model, PeftModel\n",
        "\n",
        "print('GPU Available:', torch.cuda.is_available())\n",
        "if torch.cuda.is_available():\n",
        "    print('GPU Name:', torch.cuda.get_device_name(0))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìÇ Upload Dataset\n",
        "Upload `theni_slang_dataset.csv` with columns:\n",
        "- normal_tamil\n",
        "- theni_slang"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load Dataset\n",
        "df = pd.read_csv('/content/theni_slang_dataset.csv')\n",
        "dataset = Dataset.from_pandas(df)\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load Base mBART Model\n",
        "model_name = 'facebook/mbart-large-50-many-to-many-mmt'\n",
        "\n",
        "tokenizer = MBart50TokenizerFast.from_pretrained(model_name)\n",
        "model = MBartForConditionalGeneration.from_pretrained(model_name)\n",
        "\n",
        "tokenizer.src_lang = 'ta_IN'\n",
        "model.gradient_checkpointing_enable()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Preprocessing\n",
        "def preprocess(example):\n",
        "    inputs = tokenizer(\n",
        "        example['normal_tamil'],\n",
        "        max_length=64,\n",
        "        truncation=True\n",
        "    )\n",
        "\n",
        "    with tokenizer.as_target_tokenizer():\n",
        "        labels = tokenizer(\n",
        "            example['theni_slang'],\n",
        "            max_length=64,\n",
        "            truncation=True\n",
        "        )\n",
        "\n",
        "    inputs['labels'] = labels['input_ids']\n",
        "    return inputs\n",
        "\n",
        "dataset = dataset.map(preprocess, batched=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Apply LoRA\n",
        "lora_config = LoraConfig(\n",
        "    r=8,\n",
        "    lora_alpha=16,\n",
        "    target_modules=['q_proj', 'v_proj'],\n",
        "    lora_dropout=0.1,\n",
        "    bias='none',\n",
        "    task_type='SEQ_2_SEQ_LM'\n",
        ")\n",
        "\n",
        "model = get_peft_model(model, lora_config)\n",
        "model.print_trainable_parameters()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Training Arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./theni_mbart_lora',\n",
        "    per_device_train_batch_size=2,\n",
        "    gradient_accumulation_steps=4,\n",
        "    num_train_epochs=3,\n",
        "    learning_rate=2e-4,\n",
        "    fp16=True,\n",
        "    logging_steps=50,\n",
        "    save_total_limit=1,\n",
        "    report_to='none'\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=dataset\n",
        ")\n",
        "\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save LoRA Adapter\n",
        "model.save_pretrained('./theni_mbart_lora')\n",
        "tokenizer.save_pretrained('./theni_mbart_lora')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üîÆ Inference Example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load for Inference\n",
        "base_model = MBartForConditionalGeneration.from_pretrained(model_name)\n",
        "model = PeftModel.from_pretrained(base_model, './theni_mbart_lora')\n",
        "\n",
        "text = '‡Æ®‡Ææ‡Æ©‡Øç ‡Æµ‡Æ∞‡ØÅ‡Æï‡Æø‡Æ±‡Øá‡Æ©‡Øç'\n",
        "\n",
        "tokenizer.src_lang = 'ta_IN'\n",
        "inputs = tokenizer(text, return_tensors='pt').to(model.device)\n",
        "\n",
        "generated = model.generate(\n",
        "    **inputs,\n",
        "    forced_bos_token_id=tokenizer.lang_code_to_id['ta_IN'],\n",
        "    max_length=64\n",
        ")\n",
        "\n",
        "print('Input:', text)\n",
        "print('Output:', tokenizer.decode(generated[0], skip_special_tokens=True))"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
