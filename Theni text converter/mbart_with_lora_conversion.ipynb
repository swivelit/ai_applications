{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z52IZQwqi5eU"
      },
      "source": [
        "# Theni Tamil Dialect Translator\n",
        "**Before running:**\n",
        "1. Runtime → Disconnect and delete runtime\n",
        "2. Runtime → Change runtime type → T4 GPU → Save\n",
        "3. Run each cell top to bottom"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mTkqX7Rri5eb"
      },
      "outputs": [],
      "source": [
        "# CELL 1 - Install\n",
        "!pip install -q transformers sentencepiece peft accelerate\n",
        "print('Done')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4MLsyoKGi5ek"
      },
      "outputs": [],
      "source": [
        "# CELL 2 - Imports\n",
        "import os, gc, io, torch, shutil\n",
        "import pandas as pd\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, get_linear_schedule_with_warmup\n",
        "from peft import get_peft_model, LoraConfig, TaskType, PeftModel\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f'Device: {device}')\n",
        "if torch.cuda.is_available():\n",
        "    free  = torch.cuda.mem_get_info()[0] / 1e9\n",
        "    total = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
        "    print(f'GPU: {torch.cuda.get_device_name(0)}')\n",
        "    print(f'VRAM: {total:.1f} GB total | {free:.1f} GB free')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UyXvtuLki5em"
      },
      "outputs": [],
      "source": [
        "# CELL 3 - Upload CSV\n",
        "from google.colab import files\n",
        "print('Upload your dataset.csv ...')\n",
        "uploaded = files.upload()\n",
        "filename = list(uploaded.keys())[0]\n",
        "df_raw   = pd.read_csv(io.BytesIO(uploaded[filename]))\n",
        "print(f'Uploaded: {filename}')\n",
        "print(f'Rows: {len(df_raw)}')\n",
        "print(f'Columns: {list(df_raw.columns)}')\n",
        "df_raw.head(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qle8UarTi5ep"
      },
      "outputs": [],
      "source": [
        "# CELL 4 - Settings (change column names if needed)\n",
        "SOURCE_COL = 'normal_tamil'\n",
        "TARGET_COL = 'theni_tamil'\n",
        "\n",
        "MODEL_NAME = 'facebook/nllb-200-distilled-600M'\n",
        "SRC_LANG   = 'tam_Taml'\n",
        "TGT_LANG   = 'tam_Taml'\n",
        "MAX_LENGTH = 128\n",
        "BATCH_SIZE = 8\n",
        "EPOCHS     = 20\n",
        "LR         = 3e-4\n",
        "MODEL_DIR  = '/content/best_model'\n",
        "\n",
        "df = df_raw[[SOURCE_COL, TARGET_COL]].dropna()\n",
        "df[SOURCE_COL] = df[SOURCE_COL].str.strip()\n",
        "df[TARGET_COL] = df[TARGET_COL].str.strip()\n",
        "df = df[df[SOURCE_COL].str.len() > 0]\n",
        "df = df[df[TARGET_COL].str.len() > 0]\n",
        "\n",
        "src_train, src_val, tgt_train, tgt_val = train_test_split(\n",
        "    df[SOURCE_COL].tolist(), df[TARGET_COL].tolist(),\n",
        "    test_size=0.1, random_state=42\n",
        ")\n",
        "print(f'Train: {len(src_train)} | Val: {len(src_val)}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i2GvqElKi5et"
      },
      "outputs": [],
      "source": [
        "# CELL 5 - Load model + LoRA\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "print('Loading tokenizer...')\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "\n",
        "print('Loading base model in fp16...')\n",
        "base_model = AutoModelForSeq2SeqLM.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    torch_dtype=torch.float16\n",
        ")\n",
        "base_model.config.use_cache = False\n",
        "\n",
        "print('Applying LoRA...')\n",
        "lora_config = LoraConfig(\n",
        "    task_type=TaskType.SEQ_2_SEQ_LM,\n",
        "    r=16,\n",
        "    lora_alpha=32,\n",
        "    lora_dropout=0.1,\n",
        "    target_modules=['q_proj', 'v_proj']\n",
        ")\n",
        "model = get_peft_model(base_model, lora_config)\n",
        "\n",
        "# Cast LoRA params to float32 so gradients work correctly\n",
        "for name, param in model.named_parameters():\n",
        "    if param.requires_grad:\n",
        "        param.data = param.data.float()\n",
        "\n",
        "model.to(device)\n",
        "\n",
        "total     = sum(p.numel() for p in model.parameters())\n",
        "trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f'Total params    : {total/1e6:.1f}M')\n",
        "print(f'Trainable params: {trainable/1e6:.2f}M ({100*trainable/total:.2f}%)')\n",
        "free = torch.cuda.mem_get_info()[0] / 1e9\n",
        "print(f'Free VRAM       : {free:.1f} GB')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tLT9XK9ri5eu"
      },
      "outputs": [],
      "source": [
        "# CELL 6 - Dataset\n",
        "class TheniDataset(Dataset):\n",
        "    def __init__(self, sources, targets, tokenizer, max_length=128):\n",
        "        self.sources   = sources\n",
        "        self.targets   = targets\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len   = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.sources)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        self.tokenizer.src_lang = SRC_LANG\n",
        "        encoded = self.tokenizer(\n",
        "            str(self.sources[idx]),\n",
        "            text_target=str(self.targets[idx]),\n",
        "            max_length=self.max_len,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "        return {\n",
        "            'input_ids':      encoded['input_ids'].squeeze(),\n",
        "            'attention_mask': encoded['attention_mask'].squeeze(),\n",
        "            'labels':         encoded['labels'].squeeze()\n",
        "        }\n",
        "\n",
        "train_dataset = TheniDataset(src_train, tgt_train, tokenizer, MAX_LENGTH)\n",
        "val_dataset   = TheniDataset(src_val,   tgt_val,   tokenizer, MAX_LENGTH)\n",
        "train_loader  = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True,  num_workers=2, pin_memory=True)\n",
        "val_loader    = DataLoader(val_dataset,   batch_size=BATCH_SIZE, shuffle=False, num_workers=2, pin_memory=True)\n",
        "print(f'Steps/epoch: {len(train_loader)}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B0MaZlo_i5ev"
      },
      "outputs": [],
      "source": [
        "# CELL 7 - Train\n",
        "# Only optimize trainable LoRA parameters\n",
        "optimizer = torch.optim.AdamW(\n",
        "    [p for p in model.parameters() if p.requires_grad],\n",
        "    lr=LR, weight_decay=0.01\n",
        ")\n",
        "scheduler = get_linear_schedule_with_warmup(\n",
        "    optimizer,\n",
        "    num_warmup_steps=100,\n",
        "    num_training_steps=len(train_loader) * EPOCHS\n",
        ")\n",
        "\n",
        "os.makedirs(MODEL_DIR, exist_ok=True)\n",
        "best_val_loss = float('inf')\n",
        "\n",
        "print(f'Training {EPOCHS} epochs on {device}...')\n",
        "\n",
        "for epoch in range(1, EPOCHS + 1):\n",
        "    # Train\n",
        "    model.train()\n",
        "    total_train = 0\n",
        "    for batch in tqdm(train_loader, desc=f'Epoch {epoch}/{EPOCHS} [Train]'):\n",
        "        input_ids      = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels         = batch['labels'].to(device)\n",
        "        labels[labels == tokenizer.pad_token_id] = -100\n",
        "\n",
        "        outputs = model(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            labels=labels\n",
        "        )\n",
        "        loss = outputs.loss\n",
        "        total_train += loss.item()\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "\n",
        "        del input_ids, attention_mask, labels, outputs, loss\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    # Validate\n",
        "    model.eval()\n",
        "    total_val = 0\n",
        "    with torch.no_grad():\n",
        "        for batch in val_loader:\n",
        "            input_ids      = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels         = batch['labels'].to(device)\n",
        "            labels[labels == tokenizer.pad_token_id] = -100\n",
        "            outputs = model(\n",
        "                input_ids=input_ids,\n",
        "                attention_mask=attention_mask,\n",
        "                labels=labels\n",
        "            )\n",
        "            total_val += outputs.loss.item()\n",
        "            del input_ids, attention_mask, labels, outputs\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "    avg_train = total_train / len(train_loader)\n",
        "    avg_val   = total_val   / len(val_loader)\n",
        "    print(f'Epoch {epoch:02d} | Train: {avg_train:.4f} | Val: {avg_val:.4f}')\n",
        "\n",
        "    if avg_val < best_val_loss:\n",
        "        best_val_loss = avg_val\n",
        "        model.save_pretrained(MODEL_DIR)\n",
        "        tokenizer.save_pretrained(MODEL_DIR)\n",
        "        print(f'  Saved best model (val={best_val_loss:.4f})')\n",
        "\n",
        "print('Training complete!')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kUiPQH0pi5ew"
      },
      "outputs": [],
      "source": [
        "# CELL 8 - Download model\n",
        "from google.colab import files\n",
        "shutil.make_archive('/content/theni_model', 'zip', MODEL_DIR)\n",
        "files.download('/content/theni_model.zip')\n",
        "print('Downloaded!')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fLkrpwsii5ex"
      },
      "outputs": [],
      "source": [
        "# CELL 9 - Load model for translation\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "tokenizer_t = AutoTokenizer.from_pretrained(MODEL_DIR, local_files_only=True)\n",
        "base_t      = AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME, torch_dtype=torch.float16)\n",
        "model_t     = PeftModel.from_pretrained(base_t, MODEL_DIR, local_files_only=True)\n",
        "model_t     = model_t.merge_and_unload()\n",
        "model_t.to(device)\n",
        "model_t.eval()\n",
        "print('Model ready!')\n",
        "\n",
        "def translate(text):\n",
        "    tokenizer_t.src_lang = SRC_LANG\n",
        "    inputs = tokenizer_t(\n",
        "        text, return_tensors='pt',\n",
        "        max_length=MAX_LENGTH, truncation=True\n",
        "    ).to(device)\n",
        "    target_id = tokenizer_t.convert_tokens_to_ids(TGT_LANG)\n",
        "    with torch.no_grad():\n",
        "        out = model_t.generate(\n",
        "            **inputs,\n",
        "            forced_bos_token_id=target_id,\n",
        "            num_beams=5,\n",
        "            max_length=MAX_LENGTH,\n",
        "            early_stopping=True\n",
        "        )\n",
        "    return tokenizer_t.decode(out[0], skip_special_tokens=True)\n",
        "\n",
        "tests = [\n",
        "    'நீங்கள் எப்படி இருக்கிறீர்கள்',\n",
        "    'இன்று மழை பெய்கிறது',\n",
        "    'நான் சாப்பிட போகிறேன்',\n",
        "    'அவர் வீட்டிற்கு வருகிறார்'\n",
        "]\n",
        "print('\\n--- Test Results ---')\n",
        "for t in tests:\n",
        "    print(f'Normal Tamil : {t}')\n",
        "    print(f'Theni Slang  : {translate(t)}')\n",
        "    print('-' * 50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6knZC0ski5ez"
      },
      "outputs": [],
      "source": [
        "# CELL 10 - Type your own sentence\n",
        "print('Type any Normal Tamil. Type quit to stop.\\n')\n",
        "while True:\n",
        "    text = input('Normal Tamil : ').strip()\n",
        "    if text.lower() == 'quit':\n",
        "        break\n",
        "    if text:\n",
        "        print(f'Theni Slang  : {translate(text)}\\n')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}